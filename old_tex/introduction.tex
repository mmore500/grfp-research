\section{Introduction}
In recent years, advances in artificial intelligence have conquered a number of problems that had previously proven intractable or had poorly performing solutions. These advances include text recognition \autocite{GuRecentNetworks}, the besting of human play at the board game Go by Google's AlphaGo \autocite{Silver2016MasteringSearch}, image classification \autocite{GuRecentNetworks}, and even learning artistic style by example and applying it to images \autocite{Gatys2015AStyle}. This avalanche of breakthroughs have been made by employing back-propagation to train deep neural networks (networks with many hidden layers between the input and output layers). While this method is both powerful in certain problem domains and computationally tractable, it has significant limitations including the necessity of supervised learning during the training process (each training input must be coupled with a known desired output), poor ability to train networks with recurrence (i.e. networks with memory), and the possibility of training hanging up at local maxima \autocite[pg 312, 364]{Downing2015IntelligenceSystems}. 

Evolving Artificial Neural Networks (EANN) is a neural network training approach with the potential to sidestep the aforementioned limitations of backpropagation training and, additionally, can be naturally extended to include local learning rules, allowing for online network adaptability \autocite{Tonelli2013OnNetworks}. The potential of EANN to rival both the scale and plasticity of their biological counterparts is widely recognized \autocite{Tonelli2013OnNetworks}; thus, EANN represent a route towards a more dynamic, general artificial intelligence. EANN methodology centers on the evolutionary algorithm (EA), which --- inspired by biological evolution --- uses repeated evaluation, selection, and recombination of candidate solutions to discover novel solutions to a problem. Researchers and engineers have widely demonstrated the ability of EA to provide good solutions to a wide array difficult optimization problems and to discover novel solutions beyond the reach of human ingenuity \autocite{Poli2008AProgramming}. Nonetheless, using EA to design artificial neural networks has proven a difficult task. In recent years, significant effort has been invested in studying evolvability and developing methodology to promote it. Evolvability, a term that although inconsistently and nebulously defined throughout EANN literature \autocite{Richter2015EvolvabilitySurvey} can be considered as the ability of the evolutionary process to generate useful variation in populations subjected to artificial evolution \autocite{Richter2015EvolvabilitySurvey,Reisinger2005TowardsEvolvability, Wilder2015ReconcilingEvolvability}. Attempting to promote evolvability through traditional Darwinian fitness-based selection raises a sticky conundrum: how is it possible to ``favor properties that may prove useful to a given lineage in the future, but have no present adaptive function'' \autocite{Pigliucci2008IsEvolvable}? 

Indirect genetic encoding of candidate networks and the Baldwin effect have both been proposed as potential mechanisms to promote evolvability \autocite{Reisinger2007AcquiringRepresentations,Downing2010TheNetworks}. Indirect genetic encoding has been demonstrated to produce an inherent bias towards phenotypic regularity in the evolutionary search space \autocite{Tonelli2013OnNetworks}, which enhances evolvability in problem domains with significant regularity ultimately allowing the evolutionary algorithm to discover higher quality solutions than is possible with a direct encoding \autocite{Clune2011OnRegularity}. The bias of indirect genetic encoding towards phenotypic regularity is illustrated in Figure \ref{fig:indirect_bias}. Although indirect genetic encoding yields a bias towards phenotypic regularity that promotes evolvability, most problem domains have a degree of irregularity so the performance of a highly regular network generated by indirect encoding can be further enhanced by irregular refinement \autocite{Clune2011OnRegularity}. In experiments investigation the utility of irregular refinement, it has been implemented as evolution acting on the direct representation of a solution; however, it is postulated that learning --- post-developmental phenotypic plasticity of a neural network --- might also constitute irregular refinement, allowing for the performance of a highly regular network architecture generated from an indirect genetic representation to be enhanced via irregular modifications \autocite{Clune2011OnRegularity}. 

In addition to improving the quality of individual solutions, it is thought that phenotypic plasticity --- in particular, learning --- might bias evolutionary search toward useful adaptation by incorporating information from local search of the phenotypic landscape into the fitness score of an individual; such local search effectively smooths the fitness distribution across the phenotypic search space, promoting discovery of novel beneficial phenotypic features --- peaks of high fitness in the phenotypic search space --- and ``buying evolutionary time'' until heritable scaffolding to support the adaptation originally obtained via plasticity can be evolved. This concept, illustrated in Figure \ref{fig:baldwin_effect}, is termed the Baldwin effect \autocite{Downing2010TheNetworks,Downing2009ComputationalEffect}.




While research into the Baldwin effect has suggested that local phenotypic search can bias evolutionary search toward neural architectures that provide scaffolding for phenotypic characteristics developed via local search, it not well understood what type and degree of local phenotypic search is necessary to bias evolutionary search towards solutions that have the potential to generate high performance solutions via irregular refinement. Because performing extensive irregular refinement to a candidate solution is computationally expensive, biasing evolutionary search towards neural architectures with high potential for enhancement via irregular refinement by explicitly computing the performance of the network after irregular refinement and selecting for that characteristic is not feasible. A less computationally expensive technique to estimate irregular refinement potential could improve the quality of solutions accessible via evolutionary search with indirect encodings and subsequent irregular refinement. I posit that local phenotypic search via mutation to the direct representation of candidate neural architectures might provide such a technique to bias evolutionary search toward neural architectures with high potential for irregular refinement, that serve as a platform on which direct-encoded evolutionary search, lifetime learning, or another irregular refinement technique can operate successfully. 

My research aims to tie together work on the Baldwin effect and irregular refinement by Downing \autocite{Downing2009ComputationalEffect,Downing2010TheNetworks,DowningHeterochronousBaldwinism} and Clune et al. \autocite{Clune2011OnRegularity}, respectively, to
\begin{enumerate}
  \item better characterize how phenotypic plasticity might bias evolutionary search, and
  \item develop methodology to promote discovery of neural architectures that, irrespective of prototypic fitness of the phenotype directed from an indirect genetic representation of that neural network, have the potential to give rise to networks that exhibit high fitness via irregular refinement.
\end{enumerate}
Specifically, I am to address these objectives by considering
\begin{enumerate}
  \item the organization of local phenotypic search --- the degree to which it is biased by the fitness distribution in the phenotypic space --- and
  \item how a candidate architecture's fitness is computed based on the results of that local exploration.
\end{enumerate}
in relation to the potential for enhancement via irregular refinement of networks discovered via evolutionary search. Restated, my research goal is to investigate the effectiveness of variants of Monte Carlo exploration in the local phenotypic space of candidate neural architectures at biasing evolutionary search towards the production of networks with high potential for enhancement via irregular refinement. In a series of experiments, local search will be performed via mutation to the direct-encoded representation of candidate neural architectures as in \autocite{Clune2011OnRegularity}. Phenotypic plasticity will be realized through simulated-annealing search, with the cooling rate of the search manipulated as an experimental variable. (Figure \ref{fig:local_search_types} illustrates local phenotypic simulated-annealing search for different cooling rates). Additionally, three quantification schemes for the results of the local search will be considered: reporting of the 50th, 75th, and 100th percentile fitness scores of phenotypes encountered. It is hoped that considering these search and quantification schemes will shed light on what information about local phenotype space surrounding candidate solutions is necessary to bias evolutionary search towards neural architectures with high potential for enhancement via irregular refinement, ultimately providing a more complete picture of how phenotypic plasticity might promote the evolution of neural architectures capable of learning.

% , the capability to bias evolutionary search towards solutions , essentially to employ Monte Carlo methodology to estimate potential for enhancement via irregular refinement through  be nice to have a way to bias the evolutionary search while performing only minimal irregular refinement (local search) at each fitness evaluation.

% Monte Carlo

% Ultimately, the goal is to that can traverse the regular indirect genotype space but choose genotypes that have high fitness irregular regions around them that can be accessed via learning. How can we bias the evolutionary search to make this happen? How much information about the irregular regions around regular genotype mappings... what kind of search do we have to do to make sure that we end up somewhere with high potential to be enhanced via irregular refinement (learning).

% I wonder how incorporating information about the local space into fitness evaluation affects the amount of learning that is able to take place. The potential of the performance of a network to be enhanced by irregular modification.

% The role of irregularity here

% by incorporating information gleaned from local search in the fitness landscape into the fitness score of , thereby promoting evolvability.



% The idea is to create a regular base and then  \autocite{Tonelli2013OnNetworks}

% ``suggest that HyperNEAT can benefit from a process of refinement that adjusts individual link patterns in an irregular way. While a direct encoding provides such refinement in this paper, there are other candidate refinement processes. One intriguing possibility is that lifetime adaptation via learning can play a similar role [44], [49]– [51]. Lifetime learning algorithms could adjust the overall regular patterns produced by HyperNEAT to account for necessary irregularities. Having a learning algorithm serve as the refining process may be superior to a direct encoding'' \autocite{Clune2011OnRegularity}



 
% Bias towards regularity from i
 

 
% Tie together Baldwin effect and 

% Baldwin effect!!! plasticity in biology \autocite{MoczekTheInnovation}.

% The concept of evolvability is a major focus of the field. Although there is no consensus for an exact definition of the term, evolvability is can be presented as the idea that properties beyond the immediate fitness of a solution are important to the success of the evolutionary process; a solution's potential to yield offspring of higher fitness must also be considered. Two main conceptions of evolvability have emerged: the ability to generate heritable variation \autocite{Wilder2015ReconcilingEvolvability} and the ability to canalize mutations, biasing variation towards dimensions likely to be useful in the evolutionary search \autocite{Reisinger2007AcquiringRepresentations}. 

% Attempts to promote evolvability generally fall into two categories: indirect encodings, where a developmental process maps between a compressed genotype and the phenotypic structure of a network, and selection pressure, where the rules for selecting members of the next generation are altered. Indirect encodings such as HyperNEAT are known to inherently bias the evolutionary search towards highly regular (i.e. symmetric) phenotypes, which is useful because most problem domains have a significant degree of regularity \autocite{Clune2011OnRegularity} and because this regularity promotes ``spandrels'' in evolved solutions, which allows for more generalized learning abilities when learning rules are incorporated \autocite{Tonelli2013OnNetworks}. Although most problem domains are highly regular, they do contain a degree of irregularity. It has been demonstrated that after a network is evolved indirect encoding, solutions can be further refined by allowing irregularity to be introduced via evolution with a direct encoding \autocite{Clune2011OnRegularity}. The question of irregularity because networks that can be augmented by irregular search are potentially good candidates for post-developmental modification --- learning, an avenue to introduce irregularity to a network \autocite{Clune2011OnRegularity}. It is currently thought that phenotypic plasticity in the form of learning can increase the 

% The goal of my project is to investigate the 
% The goal of my project is to investigate the relationship between a network's amenability to irregularization via direct search and local search in the phenotype space during the evolutionary process. The question is if sampling the local phenotypic space via irregularization will bias the evolutionary search towards regions of the search space that are more amenable by irregularization via FT-NEAT. In genetic regulatory networks, incorporating information about the local phenotype space of individuals leads to selection pressure for canalizing features in the genotype \autocite{Reisinger2005TowardsEvolvability}. I plan to test this both in the context of post-evolutionary direct-encoded search.

% inspired by Baldwin effect and plasticity in biology

% amount to Monte Carlo sampling of the phenotypic space surrounding an individual

% Indirect encodings favor regularity ; selection pressure   favor regularity and allow for information beyond just the configuration of the network to be stored, and selection pressure.

% How can natural selection ``favor properties hat may prove useful to a given lineage in the future, but have no present adaptive function''? \autocite{Pigliucci2008IsEvolvable}


% Evolvability is a central question... Indirect encodings generating selection pressure for evolvability.



% \begin{itemize}
% which offers the potential for online learning and more general artificial intelligence.
%   \item limitations in this technique include needing to do supervised training, -> an alternate paradigm!; also if we want a online learning and a more general artificial intelligence this is the way to go
%   \item If we want to get the Plasticity and scale of biological networks, we will need to evolve our way there
%   \item evolutionary algorithms are a well-established field, this is how they work
%   \item applying EA to artificial neural networks, indirect encodings such as hyperNEAT and selective pressure
%   \item indirect encodings bias towards regularity,
%   \item levels of search beyond just the naive evolutionary search (i.e. learning, development are both search processes) \autocite{Downing2015IntelligenceSystems}
  
%   \item The goal of my research is to investigate the interplay between the evolutionary search and the learning search
%   \item look and see if incorporating local search as in \autocite{Reisinger2005TowardsEvolvability} can affect the effectiveness of the FT-NEAT phase of HybrID (which is a stand-in for learning)
%   \item then, look and see if incorporating local search ... something... connect to Mouret \autocite{Tonelli2013OnNetworks} (move on from direct representation addition of irregularity to addition of irregularity via learning)-
%   \item HybrID (hyperNEAT and FT-NEAT) \autocite{Clune2011OnRegularity}, Baldwin effect [Downing], spandrels \autocite{Tonelli2013OnNetworks}, Neural Darwinism \autocite{Downing2015IntelligenceSystems}
  
% \end{itemize}