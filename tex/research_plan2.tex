The technique will first be trialed on a small scale experiment and then be extended to a larger scale experiment.

Organisms will have two components:
the DeepNEAT that generates the excess large neural net
the X that parses activity statistics to make decisions about pruning

Small scale experiment

use HyperNEAT to generate a large neural net

while training, log the following for each neuron
\begin{itemize}
\item average activation
\item median activation
\item correlation of activation with activation in the following layer
\item correlation of activation with loss
\item average weight in, number of weights in
\item average weight out, number of weights out
\item magnitude of backprop gradient through the neuron
\end{itemize}

genetic algorithm to evolve a vector of first order interactions parameters used to decide whether to prune or not


\autocite{song2015learning} report that looking at weight magnitudes alone was sufficient to prune networks without

every $n$ training iterations,
go through and remove each neuron that meets the criteria for removal

should we also take a connection by connection approach like below?

large scale experiment (performed with industry partners?)

CoDeepNEAT

while training, log for each overall connection between layers (i.e. path between component nodes)
\begin{itemize}
\item correlation between activation of afferent and efferent neurons
\item correlation between activation of afferent neuron and loss
\item magnitude of backprop gradient on those weights
\end{itemize}

If the connection meets criteria for removal

ideas for how to evaluate criteria for removal
\begin{itemize}
\item single layer neural network (evolved)
\item tree genetic programming on the logged statistics
\end{itemize}
